{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difficulty: Beginner**\n",
    "\n",
    "# Summary\n",
    "\n",
    "This example builds a very simple pipeline from dockerized components.  It example is presented with context [here](https://statcan.github.io/daaas/en/3-Pipelines/Kubeflow-Pipelines/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline uses a single component, *average*, defined as a docker image.  This docker image contains a single script that accepts numbers and returns their average.  The API is roughly equivalent to:\n",
    "\n",
    "```\n",
    "average.py number1 number2 ... numberN\n",
    "```\n",
    "\n",
    "which generates a file `out.txt` inside the running container with the result.  So we would expect:\n",
    "\n",
    "```\n",
    "average_docker_image 1 2 3\n",
    "cat out.txt\n",
    "```\n",
    "to print `6`.  To make this script portable, we package it up as a docker image and put that image in our container registry.  The full definition of the container is found in `/containers/average`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this image in a Kubeflow pipeline, we build a ContainerOp which specifies to Kubeflow how to do interact with our docker image (image location, how to pass arguments to it, what to return from the container, etc.).  To actually use these ContainerOp's in our pipeline, we build python factory functions like `average_op` below that create instances of ContainerOp's for us wired how we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_op(*numbers):\n",
    "    \"\"\"\n",
    "    Factory for average ContainerOps\n",
    "    \n",
    "    Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those\n",
    "    numbers to the underlying docker image for averaging\n",
    "    \n",
    "    Returns output collected from ./out.txt from inside the container\n",
    "\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if len(numbers) < 1:\n",
    "        raise ValueError(\"Must specify at least one number to take the average of\")\n",
    "        \n",
    "    return dsl.ContainerOp(\n",
    "        name=\"averge\",  # What will show up on the pipeline viewer\n",
    "        image=\"k8scc01covidacr.azurecr.io/kfp-components/average:v1\",  # The image that KFP runs to do the work\n",
    "        arguments=numbers,  # Passes each number as a separate (string) command line argument\n",
    "        # Script inside container writes the result (as a string) to out.txt, which \n",
    "        # KFP reads for us and brings back here as a string\n",
    "        file_outputs={'data': './out.txt'},  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called, this function returns an instance of ContainerOp that is configured to:\n",
    "* pass our `numbers` argument to the docker image (and thus the `average.py` script) as space separated command line arguments (`number0 number1 number2 ...`)\n",
    "* run the container\n",
    "* collect results by reading the file `./out.txt` (from inside the container) **into a string variable\\***, making the output available to downstream components in the pipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a more complex pipeline, we'd typically have multiple different functions that create different ContainerOps, but one is good for us here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*NOTE: Data passed here (arguments in and results returned) are all converted to strings.**\n",
    "\n",
    "* Arguments (our *numbers) are passed as strings to the docker container via command line arguments. \n",
    "* Results are collected from files (defined in file_outputs below) by Kubeflow Pipelines and passed back to the rest of the pipeline as strings (a string of each file).  \n",
    "\n",
    "This is fine for small, simple data (eg our numbers here).  For more complex objects, you\n",
    "must stringify them (convert to json, etc.).  For large results, it is likely better to put your results into a data store (eg: minio bucket) rather than a simple output file and then return a string path to the data rather\n",
    "than the data itself. \n",
    "\n",
    "In our case here, this string detail doesn't affect us because our data is simple.  But this could be a big deal if we wanted to return a binary numpy file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *pipeline* is a workflow of *components*.  The *pipeline* orchestrates our components (sets the order they run in, ensures that componentA passes data to componentB, etc.) to accomplish our work.  In this example, we define a pipeline that:\n",
    "\n",
    "1. takes an *average* of one group of numbers\n",
    "2. takes an *average* of a second group of numbers\n",
    "3. takes the *average* of the results (1) and (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are defined as python functions decorated by the @dsl.pipeline decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"my pipeline's name\"\n",
    ")\n",
    "def my_pipeline(a, b, c, d, e):\n",
    "    \"\"\"\n",
    "    Averaging pipeline which accepts five numbers and does some averaging operations on them\n",
    "    \"\"\"\n",
    "    # Compute averages for two groups\n",
    "    avg_1 = average_op(a, b, c)\n",
    "    avg_2 = average_op(d, e)\n",
    "    \n",
    "    # Use the results from _1 and _2 to compute an overall average\n",
    "    average_result_overall = average_op(avg_1.output, avg_2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above pipeline we create two averages:\n",
    "* `avg_1`: takes the average of parameters a, b, and c\n",
    "* `avg_2`: takes the average of parameters d, and e\n",
    "\n",
    "Our pipeline will run `avg_1` and `avg_2`, then pass their outputs to the third average operation.  That data exchange happens by using the `.output` attributes:\n",
    "\n",
    "```\n",
    "average_op(average_result_left.output, average_result_right.output)\n",
    "```\n",
    "\n",
    "This sort of chaining processes also helps Kubeflow Pipelines with the control flow.  By saying the third average needs outputs from avg_1 and avg_2, Kubeflow Pipelines wont run the last average until the others are complete.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate our python pipeline function into a definition Kubeflow Pipelines can use, we export to a YAML file.  This YAML is a reusable definition of our pipeline that describes all our logic we set above (what to run first, how to run *average*, etc.) but without any runtime particulars (such as the values of `a, b, ...`).  Unzip the YAML and take a look for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "pipeline_yaml = 'pipeline.yaml.zip'\n",
    "compiler.Compiler().compile(\n",
    "    my_pipeline,\n",
    "    pipeline_yaml\n",
    ")\n",
    "print(f\"Exported pipeline definition to {pipeline_yaml}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our above YAML file, we can now submit our pipeline. To do this, we:\n",
    "\n",
    "* define an experiment (a group of pipeline executions that we'll put it in)\n",
    "* submit an instance of our pipeline to Kubeflow Pipelines (populated by the parameters we want to investigate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"averaging-pipeline\"\n",
    "\n",
    "import kfp\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Run details](figures/average_with_docker_components__experiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an instance of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the pipeline, we specify the values we want to use for **this** run of the pipeline (we can then reuse the pipeline with new parameters later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_params = {\n",
    "    'a': 5,\n",
    "    'b': 5,\n",
    "    'c': 8,\n",
    "    'd': 10,\n",
    "    'e': 18,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "run = client.run_pipeline(\n",
    "    exp.id,  # Run inside the above experiment\n",
    "    experiment_name + '-' + time.strftime(\"%Y%m%d-%H%M%S\"),  # Give our job a name with a timestamp so its unique\n",
    "    pipeline_yaml,  # Pass the .yaml.zip we created above.  This defines the pipeline\n",
    "    params=pl_params  # Pass our parameters we want to run the pipeline with\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now click the above links and see your pipeline in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Run details](figures/average_with_docker_components__run.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
